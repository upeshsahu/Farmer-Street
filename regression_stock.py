import pandas as pa
import quandl 
import math
import numpy as np 
#for features scanling
from sklearn import preprocessing ,cross_validation,svm
#svm can we used with regression too
from sklearn.linear_model import LinearRegression

#importing data from quandl
df=quandl.get('WIKI/GOOGL')
#to see the data
df.head()

#               Open    High     Low    Close      Volume  Ex-Dividend  \
# Date                                                                   
# 2004-08-19  100.01  104.06   95.96  100.335  44659000.0          0.0   
# 2004-08-20  101.01  109.08  100.50  108.310  22834300.0          0.0   
# 2004-08-23  110.76  113.48  109.05  109.400  18256100.0          0.0   
# 2004-08-24  111.24  111.60  103.57  104.870  15247300.0          0.0   
# 2004-08-25  104.76  108.00  103.88  106.000   9188600.0          0.0   

#             Split Ratio  Adj. Open  Adj. High   Adj. Low  Adj. Close  \
# Date                                                                   
# 2004-08-19          1.0  50.159839  52.191109  48.128568   50.322842   
# 2004-08-20          1.0  50.661387  54.708881  50.405597   54.322689   
# 2004-08-23          1.0  55.551482  56.915693  54.693835   54.869377   
# 2004-08-24          1.0  55.792225  55.972783  51.945350   52.597363   
# 2004-08-25          1.0  52.542193  54.167209  52.100830   53.164113   

#             Adj. Volume  
# Date                     
# 2004-08-19   44659000.0  
# 2004-08-20   22834300.0  
# 2004-08-23   18256100.0  
# 2004-08-24   15247300.0  
# 2004-08-25    9188600.0

#col in dara ste
df=df[['Adj. Open','Adj. High','Adj. Low','Adj. Close','Adj. Volume',]]
#making new features 
df['HL_PCT']=(df['Adj. High']-df['Adj. Close'])/df['Adj. Close']*100.0
df['PCT_change']=(df['Adj. Close']-df['Adj. Open'])/df['Adj. Open']*100.0
df.head

# 2004-08-19    50.322842  3.712563    0.324968   44659000.0
# 2004-08-20    54.322689  0.710922    7.227007   22834300.0
# 2004-08-23    54.869377  3.729433   -1.227880   18256100.0
# 2004-08-24    52.597363  6.417469   -5.726357   15247300.0
# 2004-08-25    53.164113  1.886792    1.183658    9188600.0
# 2004-08-26    54.122070  0.037068    2.820391    7094800.0
# 2004-08-27    53.239345  2.326896   -1.803885    6211700.0
# 2004-08-30    51.162935  3.411430   -3.106003    5196700.0
# 2004-08-31    51.343492  1.308977    0.048866    4917800.0
# 2004-09-01    50.280210  2.713217   -2.385589    9138200.0
# 2004-09-02    50.912161  0.847207    2.442224   15118600.0
# 2004-09-03    50.159839  1.729827   -0.931154    5152400.0
# 2004-09-07    50.947269  0.413467    0.564301    5847500.0
# 2004-09-08    51.308384  0.713587    1.548541    4985600.0
# 2004-09-09    51.313400  0.390969   -0.185366    4061700.0
# 2004-09-10    52.828075  1.167758    3.804080    8698800.0
# 2004-09-13    53.916435  0.846512    0.815905    7844100.0
# 2004-09-14    55.917612  0.457440    3.769546   10828900.0
# 2004-09-15    56.173402  1.991071    1.302460   10713000.0
# 2004-09-16    57.161452  1.605686    1.450952    9266300.0
# 2004-09-17    58.926902  0.000000    2.683097    9472500.0
# 2004-09-20    59.864797  1.876676    2.060710   10628700.0
# 2004-09-21    59.102444  2.189409   -1.963394    7228700.0
# 2004-09-22    59.373280  1.089711    0.791826    7581200.0
# 2004-09-23    60.597057  1.498096    1.666106    8535600.0
# 2004-09-24    60.100525  3.563381   -0.942382    9123400.0
# 2004-09-27    59.313094  2.215457   -1.087320    7066100.0
# 2004-09-28    63.626409  0.425666    4.713165   16929000.0
# 2004-09-29    65.742942  3.005798    3.595985   30516400.0
# 2004-09-30    65.000651  2.083333   -0.230179   13758000.0
# ...                 ...       ...         ...          ...
# 2018-01-29  1186.480000  0.970939   -0.127946    1533931.0
# 2018-01-30  1177.370000  0.896914   -0.029718    1792602.0
# 2018-01-31  1182.220000  0.346805   -0.134312    1643877.0
# 2018-02-01  1181.590000  0.495942    0.476195    2774967.0
# 2018-02-02  1119.200000  1.081129   -0.729098    5798880.0
# 2018-02-05  1068.760000  4.325574   -2.893850    3742469.0
# 2018-02-06  1084.430000  0.272032    4.879205    3732527.0
# 2018-02-07  1055.410000  2.948617   -2.724499    2544683.0
# 2018-02-08  1005.600000  5.800517   -5.120439    3067173.0
# 2018-02-09  1043.430000  0.794495    1.710726    4436032.0
# 2018-02-12  1054.560000  1.044037   -0.199684    2796258.0
# 2018-02-13  1054.140000  0.671638    0.394286    1574121.0
# 2018-02-14  1072.700000  0.258227    1.743304    2029979.0
# 2018-02-15  1091.360000  0.251063    0.730075    1806206.0
# 2018-02-16  1095.500000  1.169329    0.193894    1971928.0
# 2018-02-20  1103.590000  1.150790    0.991068    1646405.0
# 2018-02-21  1113.750000  2.015713    0.419259    2024534.0
# 2018-02-22  1109.900000  1.401928   -0.828292    1386115.0
# 2018-02-23  1128.090000  0.080667    0.842973    1234539.0
# 2018-02-26  1143.700000  0.043718    1.046066    1489118.0
# 2018-02-27  1117.510000  2.392820   -2.289936    2094863.0
# 2018-02-28  1103.920000  2.149612   -1.611408    2431023.0
# 2018-03-01  1071.410000  3.720331   -3.436559    2766856.0
# 2018-03-02  1084.140000  0.253657    2.472637    2508145.0
# 2018-03-05  1094.760000  0.586430    1.542486    1432369.0
# 2018-03-06  1100.900000  0.429648   -0.108883    1169068.0
# 2018-03-07  1115.040000  0.104032    2.033272    1537429.0
# 2018-03-08  1129.380000  0.182401    1.090226    1510478.0
# 2018-03-09  1160.840000  0.013783    1.872751    2070174.0
# 2018-03-12  1165.930000  1.048948    0.075533    2129297.0

forecast_col='Adj. Close'
#filling NAN value with  -9999
df.fillna(-9999,inplace=True)

forecast_out=int(math.ceil(0.1*len(df)))
#converting 10% value to ceil and sshift them upward in y close
#introducing label to df
df['label']=df[forecast_col].shift(-forecast_out)
df.dropna(iplace=True)
df.head


#drooping the label col and copying the all
x=np.array(df.drop(['label'],1))
y=np.array(df['label'])

x=preprocessing.scale(x)

print "length of features",len(x)
print "length of labels",len(y)

x_train,x_test,y_train,y_test=cross_validation.train_test_split(x,y,test_size=0.2)


clf=LinearRegression()
clf.fit(x_train,y_train)
clf.score(x_test,y_test)

#accuracy is 0.51947515656291254

#we can use svr-suuuport vector regression to predict the output

clf=svm.SVR()
clf.fit(x_train,y_train)
print "accuracy=",clf.score(x_test,y_test)